## Mostly Human Show | Episode 6 | Testing Your Model and Knowing When to Try Again

[![YouTube video](https://img.youtube.com/vi/Y8B8FxaCOYc/0.jpg)](https://www.youtube.com/watch?v=Y8B8FxaCOYc)

In Episode 6 of the Mostly Human Show, Adrian and Shereen dive into the critical process of evaluating machine learning models post-training. Discover why merely achieving good training metrics isn't enough and learn how to properly test your modelâ€™s effectiveness against a baseline. Tune in for essential insights on when to iterate and refine your approach for better results!

---

## Key Takeaways

- **Baseline Comparison**  
  Always establish a baseline using the original model to gauge improvements from fine-tuning.

- **Data Imbalance**  
  Recognize and address data imbalance to ensure the model learns effectively from all categories.

- **Iterative Approach**  
  Focus on iterating on data first, then hyperparameters, and finally the model architecture if necessary.

- **Real-World Testing**  
  Conduct live testing in shadow mode to compare model outputs with human performance before full deployment.

---

<details>
<summary><strong>Video transcript (click here)</strong></summary>

Hello, everyone, and welcome back to the Mostly Human show, Episode 6. Today, Adrian, I heard that you finished training a model recently? I did, Shereen, yes. I trained the OpenAI GPT 20 billion parameters model on about 500 of our historical network trouble tickets. Our training went smoothly, losses look good, checkpoints were saved, so it looked good. Okay, so if it looks good, then what more is there to do? Why are we here today? Yeah, I mean, I have no idea if it's actually useful, which I guess is perfect timing for this episode, right? It is, you're right. So today, we're talking all about evaluation, evaluation and iteration. How do you actually test if your fine-tuned model that you work so hard for is better than what you started with? Right, because just because training loss went down doesn't mean it solves my actual real problem, right? Absolutely. You can have perfect metrics and then a useless model attached to it. So today, we need to talk about what is the proper evaluation. Let's do it. Okay, so the first question, when you were doing this, did you leave out any test data or data that the model has never seen? I did, yes. I set aside about 50 tickets that weren't in the training or validation sets. Okay, so that's a good start. What you have there, that's your test set. Now, before you continue and run your fine-tuned model, you need to establish a baseline. Meaning what? Well, what that means is that you need to run the original, the non-fine-tuned GPT model on those same 50 tickets and then see how it performs without your training. Oh, I see, to compare against that, basically. Yes, you need to compare against it because you need to prove that the fine-tuning that you're spending all this time doing actually helps your case. Otherwise, maybe the base model was already good enough to begin with and you just wasted your time and your GPU hours. That's actually a great point. Didn't think to do that. It is a really common mistake because people get so excited about this concept of fine-tuning to make it better. But let's walk through it and see for your ticket triage use case, what are you actually trying to evaluate here? Yes, so I want the model to categorize tickets by severity. I want them critical, high, medium and low severity and ideally identify the most likely root cause. Okay, so for a case like that, it seems like you already have your evaluation criteria. And for the base model, you would feed it a ticket and then see what it says, what it's evaluating that as. So in this case, how did it do when you tested it? Honestly, it did pretty bad. It could understand the text, but it didn't know our specific network topology. It couldn't interpret our custom SNMP traps and kept suggesting things that don't apply to our environment. Well, that's a good start, right? That's why we're fine-tuning the model in the first place, seeing where it's good, where it can use some improvement. So in this case, it might not be perfect, but that is a good baseline. Okay, so then I ran my fine-tuned model on the same 50 tickets. Okay, and then what happened after that? Way better. It understood our internal terminology, correctly identified device types, and the severity ratings were pretty accurate. How accurate would you say? Like, what was your accuracy on the severity classification? It was about 76%. Out of 50 tickets, it got 38 correct. And how do you think that that compares to your team's manual triage? What do you mean? Like, when your engineers manually triage these tickets, what's their accuracy? Because if humans get it right 95% of the time and your model gets it right around 76% of the time, that would be considered a gap. But if humans only get it right around 80% of the time, then you'd be really close. Oh, I see. I didn't think about that. Our team actually disagrees on severity sometimes, especially for edge cases. So in that case, you would need to understand what good performance would look like for your specific use case. Sometimes that 76% would be classified as amazing performance, but then other times it may not be considered enough. So for my use case, 76% would save us a lot of time, actually, even if we have to manually verify the 24%, it gets wrong. Yeah, there you go. So that is an example of a real-world evaluation. Not just is this number high, but does this number actually help? Right. So what about root cause identification? That was more subjective. Okay, so for qualitative evaluation, you need human review. So in this case, you would still need to get some of your senior engineers to read through the model's outputs and rate them manually. Something like on a scale from one to five or something like that? Yeah, that would work. So even just helpful, somewhat helpful, or not helpful, the key would be consistency with these categories and then have multiple people review the same output so that you can average out the overall scores. All right. So I've got my results, base model was bad, fine-tuned model is pretty good. What do I do with the 24% it got wrong? Well, with that 24%, that's where things start to get really valuable. So with that, you have the opportunity to analyze the failures. In this case, you need to look at what type of tickets your model is getting wrong. I actually did this. I looked into it. And most of the errors were on a specific category, wireless issues, right? It kept misclassifying them. Okay. And then why do you think that is? Probably because I didn't have many wireless tickets in my training data. Most of our tickets are routing and switching. Okay. Yeah, that would make sense. If your training data didn't represent your real-world distribution, then that would occur. So how many wireless tickets do you think were in your training set? I'd say maybe 20 out of 500. And in reality, what percentage of your tickets are wireless related? It's probably closer to 20, 25%. Okay. So you can see here, that would be your problem. Your model underperformed on wireless because it barely saw any examples. This would be considered a data imbalance, which is super common. So for my next training run, I need to add more wireless tickets, right? Yes. And you might even want to oversample them slightly. So that would mean to include proportionally more wireless examples than you have in reality, just to make sure that the model makes sure to learn these patterns as well. That makes sense. What about the other errors? You would have to look for patterns. So you want to see, are there specific device types it struggles with, specific error messages? Maybe there's a vendor that you didn't have much training data for. That's true. We're mostly a Cisco shop, but we do have some third-party switches. And yeah, the model did poorly on those tickets. Great. So in that case, because the model did poorly, you'd want to add more third-party examples. So you see the pattern? Every failure is telling you something about what to improve in your next iteration. So we spoke about iteration. Let's talk about iteration strategies. Based on your evaluation, you've got a few options for improving your model. Yes. I was going to just add more data in between. Yeah, that's option one, to iterate on that data. Add more wireless examples, more third-party configs, maybe some more edge cases. That's usually the best first step. What are the other options, actually, in this case? Well, there are two other options. Option two would be iterate on only the hyperparameters. Maybe your learning rate was too high or too low. Maybe you need more epochs or fewer. Maybe a different batch size. How do I know that's the problem versus just needing more data? It would be through trial and error. But here's a general rule. If your model is doing well on the things that it saw in training, but poorly on new examples, that would be an example of a data problem. If it's doing poorly across the board, even on training-like examples, that might be an example of a hyperparameter issue. My model does well on most categories, except wireless and third-party devices, which were underrepresented, like we've talked. So that sounds like a data issue, right? Kind of. So that would bring us to our third option. And that option's more advanced. Option three would be iterate your approach. Maybe GPT isn't the right base model at all. Maybe you need a larger model entirely or a different architecture. Would I need to try that? Not yet. You have clear data gaps to address first. Only change your approach if you've exhausted the data and the hyperparameters too much. That's actually reassuring. Spent a lot of time on this. So my next steps are add about 100 more wireless and third-party device tickets, retrain, and then re-evaluate. Yes. All of that, and this time, when you evaluate, compare your current fine-tuned model too, so that you can see the improvement iteration over iteration. Okay, so we start with the base model, retrain, we go to version one, fine-tune, and then we train again, re-evaluate, then we're at version two, fine-tune. Each one should be better, right? Yes. And we want to track everything, your data composition, your hyperparameters, evaluation scores, everything along the way, so that you can know what changed between versions. Okay, so here's my last question. Let's say my version two model gets 85% accuracy on my test set. How do I know it'll work in production? So test sets are controlled environments and the real world is very messy, right? So you need to do live testing, a combination of both. You mean like just like deploy in production? Not quite. You'd want to start with something smaller than that, like a subset of real incoming tickets, maybe 10% of them, and then run that through your model. But you wouldn't rely on that. You'd still have humans come in and do the normal full process. So like shadow mode? Exactly. Compare the model's output to what your team does, and then see if there are any weird edge cases that you find along the way that maybe the model doesn't handle. I see. How long should I run that? You'd want to run it for about a week or two, and then you want to capture different types of traffic. For example, weekday versus weekend, or busy period versus a quiet period, something with contrast. Okay. And then what about if it does well in shadow mode? What next? If it does well in shadow mode, then you can start using it for real, but still with human oversight. Maybe it auto-triages low severity tickets, but then it escalates anything marked critical for human review. So like a gradual rollout, basically. Yeah. You never want to trust your model 100% right away, especially for critical infrastructure. Yes. I'm not letting AI make production changes without human approval just yet. And that's really smart. It can still make mistakes. So keep collecting that feedback, and your model should keep improving as you get more real-world data with your iterations. All right. So to summarize, test against the baseline, analyze your failures, iterate on data first, then hyperparameters, then the approach if needed. And always validate in the real world before trusting your model in production. This has been really helpful. I feel like I actually have a plan now instead of just hoping things work. That's a good plan. That's the goal. And honestly, this iterative process is where all the magic happens. Your first model is rarely your best. So next episode, what are we covering? Well, I was thinking for our next episode, we could actually walk through a live training run and go over the steps that we've been talking about for the past couple of weeks. Oh, that would be fantastic. Perfect. So that's what we'll do. All right, everyone. Thanks for watching. If you fine-tune models and have had to iterate, let us know what issues you ran into. And what your accuracy numbers look like. We'd love to hear real use cases from you folks. And with that, we'll see you next week. Goodbye, everyone. See you next week, everyone. Bye.

</details>
